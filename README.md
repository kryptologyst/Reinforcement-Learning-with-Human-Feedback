# Reinforcement Learning with Human Feedback

A comprehensive implementation of Reinforcement Learning with Human Feedback (RLHF) featuring state-of-the-art algorithms, multiple environments, and interactive visualization tools.

## Features

- **Modern RL Algorithms**: PPO, DQN, Rainbow DQN with distributional RL
- **Multiple Environments**: RLHF simulation, GridWorld, MountainCar, CartPole
- **Interactive UI**: Streamlit dashboard for training and visualization
- **Comprehensive Logging**: TensorBoard and Weights & Biases integration
- **Configuration Management**: YAML/JSON config files
- **Checkpointing**: Model saving and loading
- **Unit Tests**: Comprehensive test suite
- **Type Hints**: Full type annotation support
- **PEP8 Compliance**: Clean, readable code

## ðŸ“ Project Structure

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/           # RL agent implementations
â”‚   â”œâ”€â”€ envs/            # Environment implementations
â”‚   â”œâ”€â”€ utils/           # Training utilities and logging
â”‚   â”œâ”€â”€ train.py        # Main training script
â”‚   â””â”€â”€ streamlit_app.py # Interactive UI
â”œâ”€â”€ configs/             # Configuration files
â”œâ”€â”€ tests/              # Unit tests
â”œâ”€â”€ notebooks/          # Jupyter notebooks
â”œâ”€â”€ logs/               # Training logs
â”œâ”€â”€ checkpoints/        # Model checkpoints
â”œâ”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ .gitignore         # Git ignore rules
â””â”€â”€ README.md          # This file
```

## ðŸ› ï¸ Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/kryptologyst/Reinforcement-Learning-with-Human-Feedback.git
   cd Reinforcement-Learning-with-Human-Feedback
   ```

2. **Create virtual environment**:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

## Quick Start

### Command Line Training

Train a PPO agent on the RLHF environment:
```bash
python src/train.py --env rlhf --agent ppo --timesteps 100000
```

Train a DQN agent on GridWorld:
```bash
python src/train.py --env gridworld --agent dqn --timesteps 50000
```

Train with custom configuration:
```bash
python src/train.py --config configs/gridworld_dqn.json
```

### Interactive UI

Launch the Streamlit dashboard:
```bash
streamlit run src/streamlit_app.py
```

Then open your browser to `http://localhost:8501` to access the interactive training interface.

### Jupyter Notebooks

Explore the notebooks in the `notebooks/` directory for detailed examples and analysis.

## Available Environments

### RLHF Environment
Simulates human feedback for prompt-response generation:
- **Prompts**: "Tell me a joke", "Say something wise", "Motivate me"
- **Responses**: Multiple options per prompt
- **Reward**: Based on human preference simulation

### GridWorld
Navigation task in a grid environment:
- **Goal**: Reach the target position
- **Obstacles**: Avoid red obstacles
- **Reward**: +10 for goal, -0.1 per step, -1 for obstacles

### MountainCar
Classic control problem:
- **Goal**: Reach the top of the mountain
- **Challenge**: Build momentum by swinging back and forth
- **Reward**: 0 for goal, -1 per step

### CartPole
Classic balancing task:
- **Goal**: Keep pole balanced
- **Actions**: Left/Right force
- **Reward**: +1 per step

## Available Agents

### PPO (Proximal Policy Optimization)
- **Type**: Policy gradient method
- **Best for**: Continuous and discrete action spaces
- **Features**: Clipped objective, value function, entropy bonus

### DQN (Deep Q-Network)
- **Type**: Value-based method
- **Best for**: Discrete action spaces
- **Features**: Experience replay, target network, epsilon-greedy

### Rainbow DQN
- **Type**: Advanced value-based method
- **Best for**: Discrete action spaces
- **Features**: Distributional RL, dueling architecture, prioritized replay

## Training and Evaluation

### Training Configuration

Create custom configurations in YAML or JSON format:

```yaml
# configs/custom_config.yaml
env_name: "rlhf"
agent_type: "ppo"
total_timesteps: 100000
eval_freq: 10000
log_freq: 1000
agent_config:
  learning_rate: 0.0003
  gamma: 0.99
  device: "cpu"
```

### Logging and Monitoring

The training system provides comprehensive logging:

- **Console Output**: Real-time training progress
- **TensorBoard**: Detailed metrics and visualizations
- **Weights & Biases**: Cloud-based experiment tracking
- **Learning Curves**: Automatic plotting of training progress

### Checkpointing

Models are automatically saved during training:
- **Frequency**: Configurable save intervals
- **Format**: PyTorch state dictionaries
- **Location**: `checkpoints/` directory
- **Metadata**: Episode number, configuration, additional data

## Testing

Run the comprehensive test suite:

```bash
# Run all tests
python -m pytest tests/ -v

# Run specific test file
python tests/test_components.py

# Run with coverage
python -m pytest tests/ --cov=src --cov-report=html
```

## Performance Examples

### RLHF Environment Results
- **PPO**: Achieves 90%+ success rate on preferred responses
- **Training Time**: ~5 minutes on CPU for 100k timesteps
- **Convergence**: Typically within 50k timesteps

### GridWorld Results
- **DQN**: Reaches goal in 80%+ of episodes
- **Rainbow DQN**: Faster convergence, higher success rate
- **Training Time**: ~2 minutes on CPU for 50k timesteps

## ðŸ”§ Advanced Usage

### Custom Environments

Create your own environment by extending the base classes:

```python
from src.envs.rlhf_env import RLHFEnvironment

class CustomEnvironment(RLHFEnvironment):
    def _calculate_reward(self, prompt: str, response: str) -> float:
        # Implement custom reward logic
        return custom_reward_function(prompt, response)
```

### Custom Agents

Implement new RL algorithms:

```python
from src.agents.modern_agents import AgentConfig

class CustomAgent:
    def __init__(self, state_dim: int, action_dim: int, config: AgentConfig):
        # Implement custom agent
        pass
    
    def get_action(self, state: np.ndarray, training: bool = True):
        # Implement action selection
        pass
    
    def update(self):
        # Implement learning update
        pass
```

### Configuration Management

Use the configuration system for reproducible experiments:

```python
from src.utils.training_utils import load_config, save_config

# Load configuration
config = load_config("configs/my_experiment.yaml")

# Modify configuration
config.total_timesteps = 200000

# Save modified configuration
save_config(config, "configs/my_experiment_modified.yaml")
```

## Troubleshooting

### Common Issues

1. **CUDA Out of Memory**:
   - Reduce batch size in agent configuration
   - Use CPU device: `--device cpu`

2. **Environment Not Found**:
   - Check environment name spelling
   - Ensure all dependencies are installed

3. **Import Errors**:
   - Verify Python path includes `src/` directory
   - Check virtual environment activation

### Debug Mode

Enable debug logging:

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make changes and add tests
4. Run tests: `python -m pytest tests/`
5. Commit changes: `git commit -m "Add feature"`
6. Push to branch: `git push origin feature-name`
7. Submit a pull request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- **OpenAI**: For RLHF research and methodology
- **Stable Baselines3**: For algorithm implementations
- **Gymnasium**: For environment framework
- **PyTorch**: For deep learning framework
- **Streamlit**: For interactive UI framework

## References

1. Schulman, J., et al. "Proximal Policy Optimization Algorithms." arXiv:1707.06347 (2017)
2. Mnih, V., et al. "Human-level control through deep reinforcement learning." Nature 518.7540 (2015)
3. Hessel, M., et al. "Rainbow: Combining improvements in deep reinforcement learning." AAAI (2018)
4. Ouyang, L., et al. "Training language models to follow instructions with human feedback." NeurIPS (2022)


# Reinforcement-Learning-with-Human-Feedback
